{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import logging as log\n",
    "log.basicConfig(level=log.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from baselines.utils import *\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw_data_dir': '../dataset/',\n",
       " 'train_path': 'train.json',\n",
       " 'test_path': 'test.json',\n",
       " 'dev_path': 'dev.json',\n",
       " 'model_name': 'bert-base-uncased',\n",
       " 'max_length': 512,\n",
       " 'models_save_dir': '/scratch/shu7bh/contract_nli/models',\n",
       " 'dataset_dir': '/scratch/shu7bh/contract_nli/dataset',\n",
       " 'batch_size': 32}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg['model_name'] = 'bert-base-uncased'\n",
    "cfg['batch_size'] = 32\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dir if not exists\n",
    "from pathlib import Path\n",
    "Path(cfg[\"models_save_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(cfg[\"dataset_dir\"]).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(cfg['model_name'])\n",
    "# bert = AutoModelForMaskedLM.from_pretrained(cfg['model_name'])\n",
    "\n",
    "# tokenizer.save_pretrained(cfg['models_save_dir'])\n",
    "# bert.save_pretrained(cfg['models_save_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpj0vvzq8d\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpj0vvzq8d/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg['models_save_dir'])\n",
    "bert = AutoModelForMaskedLM.from_pretrained(cfg['models_save_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 30523. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "ic| inp: ('hi, how are you hi, how are you hi, how are you hi, how are you hi, how are '\n",
      "          'you hi, how are you hi, how are you hi, how are you hi, how are you hi, how '\n",
      "          'are you hi, how are you hi, how are you hi, how are you hi, how are you hi, '\n",
      "          'how are you hi, how are you hi, how are you hi, how are you hi, how are you '\n",
      "          'hi, how are you hi, how are you hi, how are you hi, how are you hi, how are '\n",
      "          'you hi, how are you hi, how are you hi, how are you hi, how are you hi, how '\n",
      "          'are you hi, how are you hi, how are you hi, how are you hi, how are you hi, '\n",
      "          'how are you hi, how are you hi, how are you hi, how are you hi, how are you '\n",
      "          'hi, how are you hi, how are you hi, how are you hi, how are you hi, how are '\n",
      "          'you hi, how are you hi, how are you hi, how are you hi, how are you hi, how '\n",
      "          'are you hi, how are you hi, how are you hi, how are you hi, how are you hi, '\n",
      "          'how are you hi, how are you hi, how are you hi, how are you hi, how are you '\n",
      "          'hi, how are you hi, how are you hi, how are you hi, how are you hi, how are '\n",
      "          'you hi, how are you hi, how are you hi, how are you hi, how are you hi, how '\n",
      "          'are you hi, how are you hi, how are you hi, how are you hi, how are you hi, '\n",
      "          'how are you hi, how are you hi, how are you hi, how are you hi, how are you '\n",
      "          'hi, how are you hi, how are you hi, how are you hi, how are you hi, how are '\n",
      "          'you hi, how are you hi, how are you hi, how are you hi, how are you hi, how '\n",
      "          'are you hi, how are you hi, how are you hi, how are you hi, how are you hi, '\n",
      "          'how are you hi, how are you hi, how are you hi, how are you hi, how are you '\n",
      "          'hi, how are you hi, how are you hi, how are you hi, how are you hi, how are '\n",
      "          'you hi, how are you hi, how are you hi, how are you hi, how are you hi, how '\n",
      "          'are you hi, how are you hi, how are you hi, how are you hi, how are you hi, '\n",
      "          'how are you hi, how are you hi, how are you hi, how are you hi, how are you '\n",
      "          'hi, how are you hi, how are you hi, how are you hi, how are you hi, how are '\n",
      "          'you hi, how are you hi, how are you hi, how are you hi, how are you hi, how '\n",
      "          'are you hi, how are you hi, how are you hi, how are you hi, how are you hi, '\n",
      "          'how are you hi, how are you hi, how are you hi, how are you hi, how are you '\n",
      "          'hi, how are you hi, how are you hi, how are you hi, how are you hi, how are '\n",
      "          'you hi, how are you hi, how are you hi, how are you hi, how are you hi, how '\n",
      "          'are you hi, how are you hi, how are you hi, how are you hi, how are you hi, '\n",
      "          'how are you hi, how are you hi, how are you hi, how are you hi, how are you '\n",
      "          'hi, how are you hi, how are you hi, how are you hi, how are you hi, how are '\n",
      "          'you hi, how are you hi, how are you hi, how are you hi, how are you hi, how '\n",
      "          'are you hi, how are you hi, how are you hi, how are you hi, how are you hi, '\n",
      "          'how are you hi, how are you hi, how are you hi, how are you hi, how are you '\n",
      "          'hi, how are you hi, how are you hi, how are you hi, how are you hi, how are '\n",
      "          'you hi, how are you hi, how are you hi, how are you hi, how are you hi, how '\n",
      "          'are you hi, how are you hi, how are you hi, how are you hi, how are you hi, '\n",
      "          'how are you hi, how are you hi, how are you hi, how are you hi, how are you '\n",
      "          'hi, how are you hi, how are you hi, how are you hi, how are you hi, how are '\n",
      "          'you hi, how are you hi, how are you hi, how are you hi, how are you hi, how '\n",
      "          'are you ')\n",
      "ic| bert.config.max_position_embeddings: 512\n",
      "ic| len(inp['input_ids'][0]): 1002\n",
      "ic| tokenizer.decode(inp['input_ids'][0]): ('[CLS] hi, how are you hi, how are you hi, how are you hi, how are you hi, '\n",
      "                                            'how are you hi, how are you hi, how are you hi, how are you hi, how are you '\n",
      "                                            'hi, how are you hi, how are you hi, how are you hi, how are you hi, how are '\n",
      "                                            'you hi, how are you hi, how are you hi, how are you hi, how are you hi, how '\n",
      "                                            'are you hi, how are you hi, how are you hi, how are you hi, how are you hi, '\n",
      "                                            'how are you hi, how are you hi, how are you hi, how are you hi, how are you '\n",
      "                                            'hi, how are you hi, how are you hi, how are you hi, how are you hi, how are '\n",
      "                                            'you hi, how are you hi, how are you hi, how are you hi, how are you hi, how '\n",
      "                                            'are you hi, how are you hi, how are you hi, how are you hi, how are you hi, '\n",
      "                                            'how are you hi, how are you hi, how are you hi, how are you hi, how are you '\n",
      "                                            'hi, how are you hi, how are you hi, how are you hi, how are you hi, how are '\n",
      "                                            'you hi, how are you hi, how are you hi, how are you hi, how are you hi, how '\n",
      "                                            'are you hi, how are you hi, how are you hi, how are you hi, how are you hi, '\n",
      "                                            'how are you hi, how are you hi, how are you hi, how are you hi, how are you '\n",
      "                                            'hi, how are you hi, how are you hi, how are you hi, how are you hi, how are '\n",
      "                                            'you hi, how are you hi, how are you hi, how are you hi, how are you hi, how '\n",
      "                                            'are you hi, how are you hi, how are you hi, how are you hi, how are you hi, '\n",
      "                                            'how are you hi, how are you hi, how are you hi, how are you hi, how are you '\n",
      "                                            'hi, how are you hi, how are you hi, how are you hi, how are you hi, how are '\n",
      "                                            'you hi, how are you hi, how are you hi, how are you hi, how are you hi, how '\n",
      "                                            'are you hi, how are you hi, how are you hi, how are you hi, how are you hi, '\n",
      "                                            'how are you hi, how are you hi, how are you hi, how are you hi, how are you '\n",
      "                                            'hi, how are you hi, how are you hi, how are you hi, how are you hi, how are '\n",
      "                                            'you hi, how are you hi, how are you hi, how are you hi, how are you hi, how '\n",
      "                                            'are you hi, how are you hi, how are you hi, how are you hi, how are you hi, '\n",
      "                                            'how are you hi, how are you hi, how are you hi, how are you hi, how are you '\n",
      "                                            'hi, how are you hi, how are you hi, how are you hi, how are you hi, how are '\n",
      "                                            'you hi, how are you hi, how are you hi, how are you hi, how are you hi, how '\n",
      "                                            'are you hi, how are you hi, how are you hi, how are you hi, how are you hi, '\n",
      "                                            'how are you hi, how are you hi, how are you hi, how are you hi, how are you '\n",
      "                                            'hi, how are you hi, how are you hi, how are you hi, how are you hi, how are '\n",
      "                                            'you hi, how are you hi, how are you hi, how are you hi, how are you hi, how '\n",
      "                                            'are you hi, how are you hi, how are you hi, how are you hi, how are you hi, '\n",
      "                                            'how are you hi, how are you hi, how are you hi, how are you hi, how are you '\n",
      "                                            'hi, how are you hi, how are you hi, how are you hi, how are you hi, how are '\n",
      "                                            'you hi, how are you hi, how are you hi, how are you hi, how are you hi, how '\n",
      "                                            'are you hi, how are you hi, how are you hi, how are you hi, how are you hi, '\n",
      "                                            'how are you hi, how are you hi, how are you hi, how are you hi, how are you '\n",
      "                                            'hi, how are you hi, how are you hi, how are you hi, how are you hi, how are '\n",
      "                                            'you hi, how are you hi, how are you hi, how are you hi, how are you hi, how '\n",
      "                                            'are you hi, how are you hi, how are you hi, how are you hi, how are you hi, '\n",
      "                                            'how are you hi, how are you hi, how are you hi, how are you hi, how are you '\n",
      "                                            'hi, how are you [SEP]')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you hi, how are you [SEP]'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.add_special_tokens({'additional_special_tokens': ['[SPAN]']})\n",
    "# bert.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# inp = \"hi, how are you \" * 200\n",
    "# ic(inp)\n",
    "# inp = tokenizer(inp, return_tensors='pt', padding='max_length', truncation='do_not_truncate', max_length=bert.config.max_position_embeddings)\n",
    "\n",
    "# ic(bert.config.max_position_embeddings)\n",
    "# ic(len(inp['input_ids'][0]))\n",
    "# # decode\n",
    "# ic(tokenizer.decode(inp['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class NLIDataset(Dataset):\n",
    "    def __init__(self, documents, tokenizer, hypothesis, context_sizes, surround_character_size):\n",
    "        self.label_dict = get_labels()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.tokenizer.add_special_tokens({'additional_special_tokens': ['[SPAN]']})\n",
    "\n",
    "        data_points = []\n",
    "        contexts = [{}]\n",
    "\n",
    "        for context_size in context_sizes:\n",
    "            for i, doc in enumerate(documents):\n",
    "                ic(i)\n",
    "                char_idx = 0\n",
    "                while char_idx < len(doc['text']):\n",
    "                    ic(char_idx)\n",
    "                    document_spans = doc['spans']\n",
    "                    cur_context = {\n",
    "                        'doc_id': i,\n",
    "                        'start_char_idx': char_idx,\n",
    "                        'end_char_idx': char_idx + context_size,\n",
    "                        'spans' : [],\n",
    "                    }\n",
    "\n",
    "                    for j, (start, end) in enumerate(document_spans):\n",
    "                        ic(j)\n",
    "                        if end <= char_idx:\n",
    "                            continue\n",
    "\n",
    "                        cur_context['spans'].append({\n",
    "                            'start_char_idx': max(start, char_idx),\n",
    "                            'end_char_idx': min(end, char_idx + context_size),\n",
    "                            'marked': start >= char_idx and end <= char_idx + context_size,\n",
    "                            'span_id': j\n",
    "                        })\n",
    "\n",
    "                        if end > char_idx + context_size:\n",
    "                            break\n",
    "\n",
    "                    if cur_context == contexts[-1]:\n",
    "                        char_idx = cur_context['end_char_idx'] - surround_character_size\n",
    "                        continue\n",
    "\n",
    "                    contexts.append(cur_context)\n",
    "                    if len(cur_context['spans']) == 1 and not cur_context['spans'][0]['marked']:\n",
    "                        char_idx = cur_context['end_char_idx'] - surround_character_size\n",
    "                    else:\n",
    "                        char_idx = cur_context['spans'][-1]['start_char_idx'] - surround_character_size\n",
    "\n",
    "        contexts.pop(0)\n",
    "\n",
    "        for nda_name, nda_desc in hypothesis.items():\n",
    "            for i, context in enumerate(contexts):\n",
    "                data_point = {}\n",
    "                data_point['hypotheis'] = nda_desc\n",
    "                cur_premise = \"\"\n",
    "                data_point['marked_beg'] = context['spans'][0]['marked']\n",
    "                data_point['marked_end'] = context['spans'][-1]['marked']\n",
    "\n",
    "                if len(context['spans']) == 1:\n",
    "                    data_point['marked_end'] = True\n",
    "\n",
    "                span_labels = []\n",
    "\n",
    "                for span in context['spans']:\n",
    "                    if span['marked']:\n",
    "                        span_labels.append(int(span['span_id'] in documents[context['doc_id']]['annotation_sets'][0]['annotations'][nda_name]['spans']))\n",
    "\n",
    "                    cur_premise += ' [SPAN] '\n",
    "                    cur_premise += documents[context['doc_id']]['text'][span['start_char_idx']:span['end_char_idx']]\n",
    "\n",
    "                evidence = any(span_labels)\n",
    "\n",
    "                data_point['premise'] = cur_premise\n",
    "\n",
    "                nli_label = self.label_dict[documents[context['doc_id']]['annotation_sets'][0]['annotations'][nda_name]['choice']]\n",
    "\n",
    "                if not evidence and nli_label != self.label_dict['NotMentioned']:\n",
    "                    nli_label = self.label_dict['Ignore']\n",
    "\n",
    "                data_point['nli_label'] = torch.tensor(nli_label, dtype=torch.long)\n",
    "                data_point['span_labels'] = torch.tensor(span_labels, dtype=torch.long)\n",
    "\n",
    "                data_points.append(data_point)\n",
    "\n",
    "        self.data_points = data_points\n",
    "        self.span_token_id = self.tokenizer.convert_tokens_to_ids('[SPAN]')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_points)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_data = self.tokenizer(\n",
    "            [self.data_points[idx]['hypotheis']],\n",
    "            [self.data_points[idx]['premise']],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        tokenized_data['input_ids'] = tokenized_data['input_ids'].squeeze()\n",
    "        tokenized_data['attention_mask'] = tokenized_data['attention_mask'].squeeze()\n",
    "        tokenized_data['token_type_ids'] = tokenized_data['token_type_ids'].squeeze()\n",
    "\n",
    "        span_indices = torch.where(tokenized_data['input_ids'] == self.span_token_id)[0]\n",
    "\n",
    "        if not self.data_points[idx]['marked_beg']:\n",
    "            span_indices = span_indices[1:]\n",
    "        \n",
    "        if not self.data_points[idx]['marked_end'] or tokenized_data['attention_mask'][-1] == 0:\n",
    "            span_indices = span_indices[:-1]\n",
    "\n",
    "        return {\n",
    "            'input_ids': tokenized_data['input_ids'],\n",
    "            'attention_mask': tokenized_data['attention_mask'],\n",
    "            'token_type_ids': tokenized_data['token_type_ids'],\n",
    "            'span_indices': span_indices,\n",
    "            'nli_label': self.data_points[idx]['nli_label'],\n",
    "            'span_labels': self.data_points[idx]['span_labels'][:len(span_indices)]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(os.path.join(cfg['raw_data_dir'], cfg['train_path']))\n",
    "dev_data = load_data(os.path.join(cfg['raw_data_dir'], cfg['dev_path']))\n",
    "test_data = load_data(os.path.join(cfg['raw_data_dir'], cfg['test_path']))\n",
    "\n",
    "hypothesis = get_hypothesis(train_data)\n",
    "\n",
    "train_data = train_data['documents']\n",
    "dev_data = dev_data['documents']\n",
    "test_data = test_data['documents']\n",
    "\n",
    "train_data = train_data[:2]\n",
    "dev_data = dev_data[:2]\n",
    "test_data = test_data[:2]\n",
    "\n",
    "ic.disable()\n",
    "\n",
    "ic(len(train_data), len(dev_data), len(test_data))\n",
    "train_dataset = NLIDataset(train_data, tokenizer, hypothesis, [1000], 50)\n",
    "dev_dataset = NLIDataset(dev_data, tokenizer, hypothesis, [500], 50)\n",
    "test_dataset = NLIDataset(test_data, tokenizer, hypothesis, [500], 50)\n",
    "\n",
    "ic.enable()\n",
    "\n",
    "del train_data\n",
    "del dev_data\n",
    "del test_data\n",
    "del hypothesis\n",
    "# save the datasets\n",
    "torch.save(train_dataset, os.path.join(cfg['dataset_dir'], 'train_dataset.pt'))\n",
    "torch.save(dev_dataset, os.path.join(cfg['dataset_dir'], 'dev_dataset.pt'))\n",
    "torch.save(test_dataset, os.path.join(cfg['dataset_dir'], 'test_dataset.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datasets\n",
    "train_dataset = torch.load(os.path.join(cfg['dataset_dir'], 'train_dataset.pt'))\n",
    "dev_dataset = torch.load(os.path.join(cfg['dataset_dir'], 'dev_dataset.pt'))\n",
    "test_dataset = torch.load(os.path.join(cfg['dataset_dir'], 'test_dataset.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class ContractNLI(nn.Module):\n",
    "    def __init__(self, bert, num_labels, ignore_index):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.bert.eval()\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.embedding_dim = self.bert.config.hidden_size\n",
    "        self.num_labels = num_labels\n",
    "        self.lambda_ = 1\n",
    "        self.nli_criterion = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "        self.span_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        self.span_classifier = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, self.embedding_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.embedding_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "        self.nli_classifier = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, self.embedding_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.embedding_dim // 2, self.num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, sep_indices):\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids, output_hidden_states=True).hidden_states[-1]\n",
    "\n",
    "        span_logits = self.span_classifier(torch.gather(outputs, 1, sep_indices.unsqueeze(1).expand(-1, outputs.shape[-1]).unsqueeze(1)).squeeze(1))\n",
    "\n",
    "        nli_logits = self.nli_classifier(outputs[:, 0, :])\n",
    "\n",
    "        return span_logits, nli_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "\n",
    "# wandb.init(project=\"contract-nli\", entity=\"contract-nli-db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer\n",
    "\n",
    "class ContractNLITrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        span_label = inputs.pop('span_label')\n",
    "        nli_label = inputs.pop('nli_label')\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        span_logits, nli_logits = outputs[0], outputs[1]\n",
    "\n",
    "        span_loss = self.model.span_criterion(span_logits, span_label.reshape(-1, 1).float())\n",
    "        nli_loss = self.model.nli_criterion(nli_logits, nli_label)\n",
    "\n",
    "        if torch.isnan(nli_loss):\n",
    "            nli_loss = torch.tensor(0, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "        loss = span_loss + self.model.lambda_ * nli_loss\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            ic(inputs['input_ids'])\n",
    "            ic(nli_label)\n",
    "            ic(nli_logits)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    auto_find_batch_size=True,\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=10,             # total number of training epochs\n",
    "    # warmup_steps=10,               # number of warmup steps for learning rate scheduler\n",
    "    logging_steps=2,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-5,\n",
    "    run_name='1',\n",
    "    label_names=['nli_label', 'span_label'],\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ContractNLITrainer(\n",
    "    model=ContractNLI(bert, len(get_labels()), ignore_index=get_labels()['Ignore']).to(DEVICE),\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=dev_dataset,            # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
