{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import logging as log\n",
    "log.basicConfig(level=log.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from baselines.utils import *\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "os.environ['WANDB_ENTITY'] = 'contract-nli-db'\n",
    "os.environ['WANDB_PROJECT'] = 'contract-nli-metric'\n",
    "os.environ['WANDB_LOG_MODEL'] = 'end'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmppzan4oep\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmppzan4oep/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.defs.contract_nli_bert_train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw_data_dir': '../dataset/',\n",
       " 'train_path': 'train.json',\n",
       " 'test_path': 'test.json',\n",
       " 'dev_path': 'dev.json',\n",
       " 'model_name': 'bert-base-uncased',\n",
       " 'max_length': 512,\n",
       " 'models_save_dir': './scratch/shu7bh/contract_nli/models',\n",
       " 'dataset_dir': './scratch/shu7bh/contract_nli/dataset',\n",
       " 'results_dir': './scratch/shu7bh/contract_nli/results',\n",
       " 'trained_model_dir': './scratch/shu7bh/contract_nli/trained_model/',\n",
       " 'batch_size': 32}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cfg['model_name'] = 'nlpaueb/legal-bert-base-uncased'\n",
    "cfg['model_name'] = 'bert-base-uncased'\n",
    "cfg['trained_model_dir'] = './scratch/shu7bh/contract_nli/trained_model/'\n",
    "cfg['batch_size'] = 32\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dir if not exists\n",
    "from pathlib import Path\n",
    "Path(cfg[\"models_save_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(cfg[\"dataset_dir\"]).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/shu7bh/Courses/ANLP/Project/Contract-NLI/source_code/contract_nli_bert_train.ipynb:137: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"DEBUG:filelock:Attempting to acquire lock 139631544206608 on /home2/shu7bh/.cache/huggingface/hub/models--bert-base-uncased/blobs/45a2321a7ecfdaaf60a6c1fd7f5463994cc8907d.lock\\n\",\n"
     ]
    }
   ],
   "source": [
    "dev_data = load_data(os.path.join(cfg['raw_data_dir'], cfg['dev_path']))\n",
    "test_data = load_data(os.path.join(cfg['raw_data_dir'], cfg['test_path']))\n",
    "\n",
    "hypothesis = get_hypothesis(dev_data)\n",
    "\n",
    "dev_data = dev_data['documents']\n",
    "test_data = test_data['documents']\n",
    "\n",
    "dev_data = dev_data[:50]\n",
    "test_data = test_data[:50]\n",
    "\n",
    "ic.disable()\n",
    "\n",
    "ic(len(dev_data), len(test_data))\n",
    "dev_dataset = NLIDataset(dev_data, tokenizer, hypothesis, [1000, 1100, 1200, 1500], 50)\n",
    "test_dataset = NLIDataset(test_data, tokenizer, hypothesis, [1000, 1100, 1200, 1500], 50)\n",
    "\n",
    "ic.enable()\n",
    "\n",
    "del dev_data\n",
    "del test_data\n",
    "del hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44744\n",
      "39695\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_dataset))\n",
    "print(len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nli_weights, span_weight = get_class_weights(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nli_weights, span_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np\n",
    "def get_micro_average_precision_at_recall(y_true, y_pred, recall_level):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    return np.interp(recall_level, recall[::-1], precision[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy and sklearn.metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_score\n",
    "def calculate_micro_average_precision(y_true, y_pred):\n",
    "    \"\"\"Calculate the micro average precision score.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.array): True labels.\n",
    "        y_pred (np.array): Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        float: Micro average precision score.\n",
    "    \"\"\"\n",
    "    # Get the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    if num_classes == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # initialize the average precision score\n",
    "    average_precision = 0.0\n",
    "\n",
    "    # loop over all classes\n",
    "    for class_idx in range(num_classes):\n",
    "        # get the indices for this class\n",
    "        y_true_indices = np.where(y_true == class_idx)\n",
    "        # calculate the average precision score for this class\n",
    "        average_precision += ic(precision_score(\n",
    "            y_true[y_true_indices], y_pred[y_true_indices], average=\"micro\"\n",
    "        ))\n",
    "\n",
    "    # return the average over all classes\n",
    "    return average_precision / num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def calculate_f1_score_for_class(y_true, y_pred, class_idx):\n",
    "    \"\"\"Calculate the F1 score for a given class.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.array): True labels.\n",
    "        y_pred (np.array): Predicted labels.\n",
    "        class_idx (int): Index of the class.\n",
    "\n",
    "    Returns:\n",
    "        float: F1 score for the given class.\n",
    "    \"\"\"\n",
    "    # get the indices for the given class\n",
    "    y_true_indices = np.where(y_true == class_idx)\n",
    "    # calculate the F1 score for the given class\n",
    "    return f1_score(\n",
    "        y_true[y_true_indices], y_pred[y_true_indices], average=\"macro\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_recall(y_true, y_scores, recall_threshold):\n",
    "    precision, recall, threshold = precision_recall_curve(y_true, y_scores)\n",
    "    idx = (np.abs(recall - recall_threshold)).argmin()  # Find nearest recall value to threshold\n",
    "    ic(threshold[idx])\n",
    "    return precision[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    auto_find_batch_size=True,\n",
    "    output_dir=cfg['results_dir'],   # output directory\n",
    "    num_train_epochs=10,            # total number of training epochs\n",
    "    gradient_accumulation_steps=4,   # number of updates steps to accumulate before performing a backward/update pass\n",
    "    logging_strategy='epoch',\n",
    "    # eval_steps=0.25,\n",
    "    # save_steps=0.25,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    # fp16=True,\n",
    "    label_names=['nli_label', 'span_labels', 'data_for_metrics'],\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./scratch/shu7bh/contract_nli/trained_model/'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg['trained_model_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:git.cmd:Popen(['git', 'version'], cwd=/home2/shu7bh/Courses/ANLP/Project/Contract-NLI/source_code, universal_newlines=False, shell=None, istream=None)\n",
      "DEBUG:git.cmd:Popen(['git', 'version'], cwd=/home2/shu7bh/Courses/ANLP/Project/Contract-NLI/source_code, universal_newlines=False, shell=None, istream=None)\n",
      "DEBUG:wandb.docker.auth:Trying paths: ['/home2/shu7bh/.docker/config.json', '/home2/shu7bh/.dockercfg']\n",
      "DEBUG:wandb.docker.auth:No config file found\n",
      "DEBUG:sentry_sdk.errors:[Tracing] Create new propagation context: {'trace_id': 'e33d4e31b9324722a7db271922b990aa', 'span_id': 'a8c8bf43bf678d0b', 'parent_span_id': None, 'dynamic_sampling_context': None}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 253\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 360\n",
      "DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 522\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): storage.googleapis.com:443\n",
      "DEBUG:urllib3.connectionpool:https://storage.googleapis.com:443 \"GET /wandb-production.appspot.com/contract-nli-db/contract-nli/r0p4yqnz/artifact/629106715/wandb_manifest.json?Expires=1699211832&GoogleAccessId=gorilla-files-url-signer-man%40wandb-production.iam.gserviceaccount.com&Signature=cBgw8%2BnFIU7eRYM%2BxzP2Fe9VZSpn11ao%2Bm1x6bsT71Sep8NSxvPCa5LYhaJT6OtOBKfcLaCIqnde3ByXQDoZY8v8mDexslR6wk8VsXUjxSz9WJ8661zQrmwqiwcfop%2B1v%2B9rXNCEgWyomMKyBFTAmZmWR8LiRvPWkg5OVyCqi6puk6sTyW7euikfHiNkmZ5iOsAymDscFH4oYJDxRLHV%2FcKMR66IDsGJAp%2BKaF25nmHPGjL%2FL2Pks2Gpj6PhvypRVoQtf2AKGDrYVZ%2BJRhBmhfB%2B1KNmLy2TvQ9CYo3rLA%2BUFmh09FS%2BK5XDJXieylsiPPFzJLWsEHHHCiXPXVablw%3D%3D HTTP/1.1\" 200 684\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-r0p4yqnz:v0, 525.79MB. 3 files... \n",
      "DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 1259\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "Done. 0:0:13.9\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "api = wandb.Api()\n",
    "artifact = api.artifact('contract-nli-db/contract-nli/model-r0p4yqnz:v0', type='model')\n",
    "artifact_dir = artifact.download(cfg['trained_model_dir'])\n",
    "\n",
    "model = ContractNLI.from_pretrained(artifact_dir).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./scratch/shu7bh/contract_nli/trained_model/'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class ContractNLIMetricTrainer(ContractNLITrainer):\n",
    "    def __init__(self, *args, data_collator=None, **kwargs):\n",
    "        super().__init__(*args, data_collator=data_collator, **kwargs)\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None):\n",
    "        self.model.eval()\n",
    "        self.dataloader = ic(self.get_eval_dataloader(eval_dataset))\n",
    "\n",
    "        eval_nli_labels = []\n",
    "        eval_nli_preds = []\n",
    "        true_labels_per_span = {}\n",
    "        probs_per_span = {}\n",
    "\n",
    "        nli_metrics = {}\n",
    "\n",
    "        for inputs in tqdm(self.dataloader):\n",
    "            inputs = self._prepare_inputs(inputs)\n",
    "            span_labels = inputs.pop('span_labels')\n",
    "            nli_labels = inputs.pop('nli_label')\n",
    "            data_for_metrics = inputs.pop('data_for_metrics')\n",
    "\n",
    "            span_indices_to_consider = torch.where(span_labels != -1)[0]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                span_logits, nli_logits = outputs[0], outputs[1]\n",
    "\n",
    "                span_labels = span_labels.float()\n",
    "                span_logits = span_logits.float()\n",
    "                \n",
    "                span_labels = span_labels.view(-1)\n",
    "                span_logits = span_logits.view(-1)\n",
    "\n",
    "                # start_index = 0\n",
    "                \n",
    "                indices_considered = 0 # total number of span indices considered\n",
    "\n",
    "                # find the corresponding span index in data_for_metrics['span_ids'] considering -1 to be padding index\n",
    "                # ic(span_index)\n",
    "                for i, span_index_row in enumerate(data_for_metrics['span_ids']):\n",
    "                    current_index = 0 # current row's first -1 index\n",
    "                    ic(span_index_row)\n",
    "                    first_minus_one_index = torch.where(span_index_row == -1)[0]\n",
    "                    ic(first_minus_one_index)\n",
    "                    if len(first_minus_one_index) == 0:\n",
    "                        first_minus_one_index = len(span_index_row)\n",
    "                    else:\n",
    "                        first_minus_one_index = first_minus_one_index[0].item()\n",
    "\n",
    "                    key = str(data_for_metrics['doc_id'][i].item())+ '-' + str(data_for_metrics['hypothesis_id'][i].item())\n",
    "\n",
    "                    # mask span_labels and span_logits for the current row\n",
    "                    mask = span_labels[indices_considered:indices_considered+first_minus_one_index] != -1\n",
    "                    span_logits_masked = span_logits[indices_considered:indices_considered+first_minus_one_index][mask]\n",
    "\n",
    "                    spans_contribution = torch.sum(torch.sigmoid(span_logits_masked)) / (len(span_logits_masked)) \n",
    "\n",
    "                    if key in nli_metrics:\n",
    "                        nli_metrics[key]['spans_contribution'].append(spans_contribution)\n",
    "                        nli_metrics[key]['nli_logits'].append(nli_logits[i])\n",
    "                    else:\n",
    "                        nli_metrics[key] = {}\n",
    "                        nli_metrics[key]['true_nli_labels'] = nli_labels[i]\n",
    "                        nli_metrics[key]['spans_contribution'] = [spans_contribution]\n",
    "                        nli_metrics[key]['nli_logits'] = [nli_logits[i]]\n",
    "                    \n",
    "                    current_index = first_minus_one_index\n",
    "                    indices_considered += current_index\n",
    "                    \n",
    "                    ic(indices_considered)\n",
    "                    ic(current_index)\n",
    "                    cnt = 0 # count to keep track of the number of span indices added in dictionary\n",
    "                    \n",
    "                    for span_index in span_indices_to_consider:\n",
    "\n",
    "                        if span_index < indices_considered:\n",
    "                            cnt += 1\n",
    "                            value_index = span_index - (indices_considered - current_index)\n",
    "                            doc_id = data_for_metrics['doc_id'][i]\n",
    "                            hypothesis_id = data_for_metrics['hypothesis_id'][i]\n",
    "                            span_id = data_for_metrics['span_ids'][i][value_index]\n",
    "                            key = str(doc_id)+ '-' + str(hypothesis_id)+ '-' + str(span_id)\n",
    "                            true_labels_per_span[key] = span_labels[span_index]\n",
    "                            if key in probs_per_span:\n",
    "                                probs_per_span[key].append(torch.sigmoid(span_logits[span_index]))\n",
    "                                # probs_per_span[key].append(span_logits[value_index])\n",
    "                            else:\n",
    "                                probs_per_span[key] = [torch.sigmoid(span_logits[span_index])]\n",
    "                                # probs_per_span[key] = [span_logits[value_index]]\n",
    "                        else: \n",
    "                            break \n",
    "                    \n",
    "                    span_indices_to_consider = span_indices_to_consider[cnt:]\n",
    "\n",
    "                # eval_span_preds = torch.tensor(eval_span_preds.squeeze(1), dtype=torch.long)\n",
    "\n",
    "                nli_preds = torch.argmax(torch.softmax(nli_logits, dim=1), dim=1)\n",
    "                eval_nli_labels.extend(nli_labels.cpu().numpy())\n",
    "                eval_nli_preds.extend(nli_preds.cpu().numpy())\n",
    "\n",
    "        eval_span_labels = []\n",
    "        eval_span_preds = []\n",
    "\n",
    "        for key in true_labels_per_span:\n",
    "            eval_span_labels.append(true_labels_per_span[key].item())\n",
    "            eval_span_preds.append(torch.mean(torch.stack(probs_per_span[key])).item())\n",
    "\n",
    "        ##### For NLI probablities #####\n",
    "\n",
    "        # for key in nli_metrics:\n",
    "        #     nli_metrics[key]['nli_logits'] = torch.stack(nli_metrics[key]['nli_logits'])\n",
    "        #     nli_metrics[key]['spans_contribution'] = torch.stack(nli_metrics[key]['spans_contribution'])\n",
    "\n",
    "        #     span_sum = torch.sum(nli_metrics[key]['spans_contribution'])\n",
    "        #     spans_contribution = nli_metrics[key]['spans_contribution'].transpose(0, -1) @ nli_metrics[key]['nli_logits']\n",
    "\n",
    "        #     eval_nli_preds.append(torch.argmax(torch.softmax(spans_contribution/span_sum, dim=0)).item())\n",
    "        #     eval_nli_labels.append(nli_metrics[key]['true_nli_labels'].item())\n",
    "\n",
    "        ##### END #####\n",
    "\n",
    "        eval_nli_acc = accuracy_score(eval_nli_labels, eval_nli_preds)\n",
    "\n",
    "        ic.enable()\n",
    "        ic(list(zip(eval_span_labels, eval_span_preds)))\n",
    "        ic(len(eval_span_labels), len(eval_span_preds))\n",
    "        ic(sum(eval_span_labels), sum(eval_span_preds))\n",
    "\n",
    "        # find threshold for 80% recall\n",
    "        # precision, recall, thresholds = precision_recall_curve(eval_span_labels, eval_span_preds)\n",
    "\n",
    "\n",
    "        mAP = (average_precision_score(eval_span_labels, eval_span_preds, pos_label=0) + average_precision_score(eval_span_labels, eval_span_preds, pos_label=1))/2\n",
    "\n",
    "        # mAP = average_precision_score(torch.tensor(true_span_labels), torch.tensor(pred_span_labels))\n",
    "        precision_at_80_recall = precision_at_recall(torch.tensor(eval_span_labels), torch.tensor(eval_span_preds), 0.8)\n",
    "        f1_score_for_entailment = calculate_f1_score_for_class(torch.tensor(eval_nli_labels), torch.tensor(eval_nli_preds), get_labels()['Entailment'])\n",
    "        f1_score_for_contradiction = calculate_f1_score_for_class(torch.tensor(eval_nli_labels), torch.tensor(eval_nli_preds), get_labels()['Contradiction'])\n",
    "        \n",
    "        return {\n",
    "            'mAP' : mAP,\n",
    "            'precision_at_80_recall' : precision_at_80_recall,\n",
    "            'nli_acc': eval_nli_acc,\n",
    "            'f1_score_for_entailment': f1_score_for_entailment,\n",
    "            'f1_score_for_contradiction': f1_score_for_contradiction\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ContractNLIMetricTrainer(\n",
    "    model=model,                          # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    # train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=dev_dataset,            # evaluation dataset\n",
    "    data_collator=ContractNLIMetricTrainer.collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 104/1399 [01:13<14:10,  1.52it/s]"
     ]
    }
   ],
   "source": [
    "ic.disable()\n",
    "# ic.enable()\n",
    "results = trainer.evaluate()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
