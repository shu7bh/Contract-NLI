{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import logging as log\n",
    "log.basicConfig(level=log.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from baselines.utils import *\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "os.environ['WANDB_ENTITY'] = 'contract-nli-db'\n",
    "os.environ['WANDB_PROJECT'] = 'contract-nli-metric'\n",
    "os.environ['WANDB_LOG_MODEL'] = 'end'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.contract_nli_bert_train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw_data_dir': '../dataset/',\n",
       " 'train_path': 'train.json',\n",
       " 'test_path': 'test.json',\n",
       " 'dev_path': 'dev.json',\n",
       " 'model_name': 'bert-base-uncased',\n",
       " 'max_length': 512,\n",
       " 'models_save_dir': '/scratch/shu7bh/contract_nli/models',\n",
       " 'dataset_dir': '/scratch/shu7bh/contract_nli/dataset',\n",
       " 'results_dir': '/scratch/shu7bh/contract_nli/results',\n",
       " 'trained_model_dir': '/scratch/shu7bh/contract_nli/trained_model/',\n",
       " 'batch_size': 32}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cfg['model_name'] = 'nlpaueb/legal-bert-base-uncased'\n",
    "cfg['model_name'] = 'bert-base-uncased'\n",
    "cfg['trained_model_dir'] = '/scratch/shu7bh/contract_nli/trained_model/'\n",
    "cfg['batch_size'] = 32\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dir if not exists\n",
    "from pathlib import Path\n",
    "Path(cfg[\"models_save_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(cfg[\"dataset_dir\"]).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/shu7bh/Courses/ANLP/Project/Contract-NLI/source_code/contract_nli_bert_train.ipynb:137: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"metadata\": {},\n"
     ]
    }
   ],
   "source": [
    "train_data = load_data(os.path.join(cfg['raw_data_dir'], cfg['train_path']))\n",
    "dev_data = load_data(os.path.join(cfg['raw_data_dir'], cfg['dev_path']))\n",
    "test_data = load_data(os.path.join(cfg['raw_data_dir'], cfg['test_path']))\n",
    "\n",
    "hypothesis = get_hypothesis(train_data)\n",
    "\n",
    "train_data = train_data['documents']\n",
    "dev_data = dev_data['documents']\n",
    "test_data = test_data['documents']\n",
    "\n",
    "train_data = train_data[:2]\n",
    "dev_data = dev_data[:2]\n",
    "test_data = test_data[:2]\n",
    "\n",
    "ic.disable()\n",
    "\n",
    "ic(len(train_data), len(dev_data), len(test_data))\n",
    "train_dataset = NLIDataset(train_data, tokenizer, hypothesis, [1000, 1100, 1200, 1500], 50)\n",
    "dev_dataset = NLIDataset(dev_data, tokenizer, hypothesis, [1000, 1100, 1200, 1500], 50)\n",
    "test_dataset = NLIDataset(test_data, tokenizer, hypothesis, [1000, 1100, 1200, 1500], 50)\n",
    "\n",
    "ic.enable()\n",
    "\n",
    "del train_data\n",
    "del dev_data\n",
    "del test_data\n",
    "del hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1734\n",
      "3026\n",
      "2346\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(dev_dataset))\n",
    "print(len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nli_weights, span_weight = get_class_weights(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.8757575757575757, 0.7031630170316302, 2.2936507936507935],\n",
       " 36.25806451612903)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli_weights, span_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np\n",
    "def get_micro_average_precision_at_recall(y_true, y_pred, recall_level):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    return np.interp(recall_level, recall[::-1], precision[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy and sklearn.metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_score\n",
    "def calculate_micro_average_precision(y_true, y_pred):\n",
    "    \"\"\"Calculate the micro average precision score.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.array): True labels.\n",
    "        y_pred (np.array): Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        float: Micro average precision score.\n",
    "    \"\"\"\n",
    "    # Get the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    if num_classes == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # initialize the average precision score\n",
    "    average_precision = 0.0\n",
    "\n",
    "    # loop over all classes\n",
    "    for class_idx in range(num_classes):\n",
    "        # get the indices for this class\n",
    "        y_true_indices = np.where(y_true == class_idx)\n",
    "        # calculate the average precision score for this class\n",
    "        average_precision += ic(precision_score(\n",
    "            y_true[y_true_indices], y_pred[y_true_indices], average=\"micro\"\n",
    "        ))\n",
    "\n",
    "    # return the average over all classes\n",
    "    return average_precision / num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def calculate_f1_score_for_class(y_true, y_pred, class_idx):\n",
    "    \"\"\"Calculate the F1 score for a given class.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.array): True labels.\n",
    "        y_pred (np.array): Predicted labels.\n",
    "        class_idx (int): Index of the class.\n",
    "\n",
    "    Returns:\n",
    "        float: F1 score for the given class.\n",
    "    \"\"\"\n",
    "    # get the indices for the given class\n",
    "    y_true_indices = np.where(y_true == class_idx)\n",
    "    # calculate the F1 score for the given class\n",
    "    return f1_score(\n",
    "        y_true[y_true_indices], y_pred[y_true_indices], average=\"macro\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    auto_find_batch_size=True,\n",
    "    output_dir=cfg['results_dir'],   # output directory\n",
    "    num_train_epochs=10,            # total number of training epochs\n",
    "    gradient_accumulation_steps=4,   # number of updates steps to accumulate before performing a backward/update pass\n",
    "    logging_strategy='epoch',\n",
    "    # eval_steps=0.25,\n",
    "    # save_steps=0.25,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    label_names=['nli_label', 'span_labels', 'data_for_metrics'],\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/shu7bh/contract_nli/trained_model/'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg['trained_model_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:git.cmd:Popen(['git', 'version'], cwd=/home2/shu7bh/Courses/ANLP/Project/Contract-NLI/source_code, universal_newlines=False, shell=None, istream=None)\n",
      "DEBUG:git.cmd:Popen(['git', 'version'], cwd=/home2/shu7bh/Courses/ANLP/Project/Contract-NLI/source_code, universal_newlines=False, shell=None, istream=None)\n",
      "DEBUG:wandb.docker.auth:Trying paths: ['/home2/shu7bh/.docker/config.json', '/home2/shu7bh/.dockercfg']\n",
      "DEBUG:wandb.docker.auth:No config file found\n",
      "DEBUG:sentry_sdk.errors:[Tracing] Create new propagation context: {'trace_id': '62df3a1d0ca2457ebad056ae9ec7994f', 'span_id': '8256824dea33eeff', 'parent_span_id': None, 'dynamic_sampling_context': None}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 253\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 367\n",
      "DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 520\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): storage.googleapis.com:443\n",
      "DEBUG:urllib3.connectionpool:https://storage.googleapis.com:443 \"GET /wandb-production.appspot.com/contract-nli-db/contract-nli-legal/swzofvm2/artifact/627548977/wandb_manifest.json?Expires=1699093540&GoogleAccessId=gorilla-files-url-signer-man%40wandb-production.iam.gserviceaccount.com&Signature=Mkih4YfIlROmfXJw9raiY%2Boj%2BLzmpCU1jASnSt297M0Nh0qUzSfocFop3mtPicaN2G%2FhVYOGY0jM6QguxtQHc1W5hEbReRh8cIFtoGwyECiB2fQNpBDBl5g9VQVOz9rx39WrJn1EWsOk%2Bs%2BFSd8TXgLNCca0i1j6aoj%2BC0d%2FFBYWg%2FGb37MnR2s4JSVBlZT4HqU8FZlSjZUmxalLCPXp44907bczAD6PXvt15umze9oXzmWtcjDM7W1S%2BroE8EwU3RbuEBxsnBiixJMqggau6ImJJlF2Ip7K8pPyKY8pHiIkmm9NIWBQ4eSvqlT2iLUugg2v6jcItWSYX4SXWpXMBQ%3D%3D HTTP/1.1\" 200 684\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-swzofvm2:v0, 471.79MB. 3 files... \n",
      "DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 1257\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "Done. 0:0:3.4\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /nlpaueb/legal-bert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "Some weights of ContractNLI were not initialized from the model checkpoint at /scratch/shu7bh/contract_nli/trained_model/ and are newly initialized: ['span_criterion.pos_weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "api = wandb.Api()\n",
    "artifact = api.artifact('contract-nli-db/contract-nli-legal/model-swzofvm2:v0', type='model')\n",
    "artifact_dir = artifact.download(cfg['trained_model_dir'])\n",
    "\n",
    "model = ContractNLI.from_pretrained(artifact_dir).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/shu7bh/contract_nli/trained_model/'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class ContractNLIMetricTrainer(ContractNLITrainer):\n",
    "    def __init__(self, *args, data_collator=None, **kwargs):\n",
    "        super().__init__(*args, data_collator=data_collator, **kwargs)\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None):\n",
    "        ic(eval_dataset)\n",
    "        self.model.eval()\n",
    "        self.dataloader = ic(self.get_eval_dataloader(eval_dataset))\n",
    "\n",
    "        eval_span_labels = []\n",
    "        eval_span_preds = []\n",
    "        eval_nli_labels = []\n",
    "        eval_nli_preds = []\n",
    "\n",
    "        for inputs in tqdm(self.dataloader):\n",
    "            inputs = self._prepare_inputs(inputs)\n",
    "            ic(inputs['input_ids'].shape)\n",
    "            span_labels = inputs.pop('span_labels')\n",
    "            nli_labels = inputs.pop('nli_label')\n",
    "            inputs.pop('data_for_metrics')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                span_logits, nli_logits = outputs[0], outputs[1]\n",
    "                \n",
    "                mask = span_labels != -1\n",
    "                span_labels = span_labels[mask]\n",
    "                span_logits = span_logits[mask]\n",
    "                \n",
    "                span_labels = span_labels.float()\n",
    "                span_logits = span_logits.float()\n",
    "                \n",
    "                span_labels = span_labels.view(-1)\n",
    "                span_logits = span_logits.view(-1)\n",
    "\n",
    "                span_preds = ic(torch.sigmoid(span_logits)) >= 0.5\n",
    "                # eval_span_preds = torch.tensor(eval_span_preds.squeeze(1), dtype=torch.long)\n",
    "                nli_preds = torch.argmax(torch.softmax(nli_logits, dim=1), dim=1)\n",
    "\n",
    "                eval_span_labels.extend(span_labels.cpu().numpy())\n",
    "                eval_span_preds.extend(span_preds.cpu().numpy())\n",
    "                eval_nli_labels.extend(nli_labels.cpu().numpy())\n",
    "                eval_nli_preds.extend(nli_preds.cpu().numpy())\n",
    "\n",
    "\n",
    "        eval_nli_acc = accuracy_score(eval_nli_labels, eval_nli_preds)\n",
    "\n",
    "        ic.enable()\n",
    "        ic(len(eval_span_labels), len(eval_span_preds))\n",
    "\n",
    "        ic(sum(eval_span_labels), sum(eval_span_preds))\n",
    "\n",
    "        mAP = calculate_micro_average_precision(torch.tensor(eval_span_labels), torch.tensor(eval_span_preds))        \n",
    "\n",
    "        # mAP = average_precision_score(torch.tensor(true_span_labels), torch.tensor(pred_span_labels))\n",
    "        precision_at_80_recall = get_micro_average_precision_at_recall(torch.tensor(eval_span_labels), torch.tensor(eval_span_preds), 0.8)\n",
    "        f1_score_for_entailment = calculate_f1_score_for_class(torch.tensor(eval_nli_labels), torch.tensor(eval_nli_preds), get_labels()['Entailment'])\n",
    "        f1_score_for_contradiction = calculate_f1_score_for_class(torch.tensor(eval_nli_labels), torch.tensor(eval_nli_preds), get_labels()['Contradiction'])\n",
    "        \n",
    "        return {\n",
    "            'mAP' : mAP,\n",
    "            'precision_at_80_recall' : precision_at_80_recall,\n",
    "            'nli_acc': eval_nli_acc,\n",
    "            'f1_score_for_entailment': f1_score_for_entailment,\n",
    "            'f1_score_for_contradiction': f1_score_for_contradiction\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/shu7bh/miniforge3/envs/nli/lib/python3.11/site-packages/transformers/trainer.py:366: FutureWarning: `Trainer` requires either a `model` or `model_init` argument, but not both. `model_init` will overwrite your model when calling the `train` method. This will become a fatal error in the next release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = ContractNLIMetricTrainer(\n",
    "    model=model,                          # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=dev_dataset,            # evaluation dataset\n",
    "    data_collator=ContractNLIMetricTrainer.collate_fn,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.001)],\n",
    "    model_init=model_init,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic.disable()\n",
    "results = trainer.evaluate()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
