{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import logging as log\n",
    "log.basicConfig(level=log.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from baselines.utils import *\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "os.environ['WANDB_ENTITY'] = 'contract-nli-db'\n",
    "os.environ['WANDB_PROJECT'] = 'contract-nli-metric'\n",
    "os.environ['WANDB_LOG_MODEL'] = 'end'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw_data_dir': '../dataset/',\n",
       " 'train_path': 'train.json',\n",
       " 'test_path': 'test.json',\n",
       " 'dev_path': 'dev.json',\n",
       " 'model_name': 'bert-base-uncased',\n",
       " 'max_length': 512,\n",
       " 'models_save_dir': '/scratch/shu7bh/contract_nli/models',\n",
       " 'dataset_dir': '/scratch/shu7bh/contract_nli/dataset',\n",
       " 'results_dir': '/scratch/shu7bh/contract_nli/results',\n",
       " 'trained_model_dir': '/scratch/shu7bh/contract_nli/trained_model/',\n",
       " 'batch_size': 32}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg['model_name'] = 'bert-base-uncased'\n",
    "cfg['trained_model_dir'] = '/scratch/shu7bh/contract_nli/trained_model/'\n",
    "cfg['batch_size'] = 32\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dir if not exists\n",
    "from pathlib import Path\n",
    "Path(cfg[\"models_save_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(cfg[\"dataset_dir\"]).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypothesis_idx(hypothesis_name):\n",
    "    return int(hypothesis_name.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import torch\n",
    "\n",
    "class NLIDataset(Dataset):\n",
    "    def __init__(self, documents, tokenizer, hypothesis, context_sizes, surround_character_size):\n",
    "        label_dict = get_labels()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.tokenizer.add_special_tokens({'additional_special_tokens': ['[SPAN]']})\n",
    "\n",
    "        data_points = []\n",
    "        contexts = [{}]\n",
    "\n",
    "        for context_size in context_sizes:\n",
    "            for i, doc in enumerate(documents):\n",
    "                char_idx = 0\n",
    "                while char_idx < len(doc['text']):\n",
    "                    ic(char_idx)\n",
    "                    document_spans = doc['spans']\n",
    "                    cur_context = {\n",
    "                        'doc_id': i,\n",
    "                        'start_char_idx': char_idx,\n",
    "                        'end_char_idx': char_idx + context_size,\n",
    "                        'spans' : [],\n",
    "                    }\n",
    "\n",
    "                    for j, (start, end) in enumerate(document_spans):\n",
    "                        if end <= char_idx:\n",
    "                            continue\n",
    "\n",
    "                        cur_context['spans'].append({\n",
    "                            'start_char_idx': max(start, char_idx),\n",
    "                            'end_char_idx': min(end, char_idx + context_size),\n",
    "                            'marked': start >= char_idx and end <= char_idx + context_size,\n",
    "                            'span_id': j\n",
    "                        })\n",
    "\n",
    "                        if end > char_idx + context_size:\n",
    "                            break\n",
    "\n",
    "                    if cur_context == contexts[-1]:\n",
    "                        char_idx = cur_context['end_char_idx'] - surround_character_size\n",
    "                        continue\n",
    "\n",
    "                    contexts.append(cur_context)\n",
    "                    if len(cur_context['spans']) == 1 and not cur_context['spans'][0]['marked']:\n",
    "                        char_idx = cur_context['end_char_idx'] - surround_character_size\n",
    "                    else:\n",
    "                        char_idx = cur_context['spans'][-1]['start_char_idx'] - surround_character_size\n",
    "\n",
    "        contexts.pop(0)\n",
    "\n",
    "        for nda_name, nda_desc in hypothesis.items():\n",
    "            for i, context in enumerate(contexts):\n",
    "\n",
    "                nli_label = label_dict[documents[context['doc_id']]['annotation_sets'][0]['annotations'][nda_name]['choice']]\n",
    "\n",
    "                if nli_label == label_dict['NotMentioned'] and random.random() > 0.04:\n",
    "                    continue\n",
    "\n",
    "                if nli_label == label_dict['Entailment'] and random.random() > 0.34:\n",
    "                    continue\n",
    "\n",
    "                data_point = {}\n",
    "                data_point['hypotheis'] = nda_desc\n",
    "                cur_premise = \"\"\n",
    "                data_point['marked_beg'] = context['spans'][0]['marked']\n",
    "                data_point['marked_end'] = context['spans'][-1]['marked']\n",
    "                doc_id = context['doc_id']\n",
    "                hypothesis_id = get_hypothesis_idx(nda_name)\n",
    "                span_ids = []\n",
    "\n",
    "                if len(context['spans']) == 1:\n",
    "                    data_point['marked_end'] = True\n",
    "\n",
    "                span_labels = []\n",
    "\n",
    "                for span in context['spans']:\n",
    "                    val = int(span['span_id'] in documents[context['doc_id']]['annotation_sets'][0]['annotations'][nda_name]['spans'])\n",
    "\n",
    "                    if val == 0 and random.random() > 0.3:\n",
    "                        continue\n",
    "\n",
    "                    if span['marked']:\n",
    "                        span_labels.append(val)\n",
    "                        span_ids.append(span['span_id'])\n",
    "\n",
    "                    cur_premise += ' [SPAN] '\n",
    "                    cur_premise += documents[context['doc_id']]['text'][span['start_char_idx']:span['end_char_idx']]\n",
    "\n",
    "                evidence = any(span_labels)\n",
    "\n",
    "                data_point['premise'] = cur_premise\n",
    "\n",
    "                # nli_label = label_dict[documents[context['doc_id']]['annotation_sets'][0]['annotations'][nda_name]['choice']]\n",
    "\n",
    "                if not evidence and nli_label != label_dict['NotMentioned']:\n",
    "                    continue\n",
    "\n",
    "                data_point['nli_label'] = torch.tensor(nli_label, dtype=torch.long)\n",
    "                data_point['span_labels'] = torch.tensor(span_labels, dtype=torch.long)\n",
    "                data_point['doc_id'] = torch.tensor(doc_id, dtype=torch.long)\n",
    "                data_point['hypothesis_id'] = torch.tensor(hypothesis_id, dtype=torch.long)\n",
    "                data_point['span_ids'] = torch.tensor(span_ids, dtype=torch.long)\n",
    "\n",
    "                data_points.append(data_point)\n",
    "\n",
    "        self.data_points = data_points\n",
    "        self.span_token_id = self.tokenizer.convert_tokens_to_ids('[SPAN]')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_points)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_data = self.tokenizer(\n",
    "            [self.data_points[idx]['hypotheis']],\n",
    "            [self.data_points[idx]['premise']],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        tokenized_data['input_ids'] = tokenized_data['input_ids'].squeeze()\n",
    "        tokenized_data['attention_mask'] = tokenized_data['attention_mask'].squeeze()\n",
    "        tokenized_data['token_type_ids'] = tokenized_data['token_type_ids'].squeeze()\n",
    "\n",
    "        span_indices = torch.where(tokenized_data['input_ids'] == self.span_token_id)[0]\n",
    "\n",
    "        if not self.data_points[idx]['marked_beg']:\n",
    "            span_indices = span_indices[1:]\n",
    "        \n",
    "        if not self.data_points[idx]['marked_end'] or tokenized_data['attention_mask'][-1] == 0:\n",
    "            span_indices = span_indices[:-1]\n",
    "        \n",
    "        span_ids = self.data_points[idx]['span_ids']\n",
    "        span_ids = span_ids[:len(span_indices)]\n",
    "\n",
    "        return {\n",
    "            'input_ids': tokenized_data['input_ids'],\n",
    "            'attention_mask': tokenized_data['attention_mask'],\n",
    "            'token_type_ids': tokenized_data['token_type_ids'],\n",
    "            'span_indices': span_indices,\n",
    "            'nli_label': self.data_points[idx]['nli_label'],\n",
    "            'span_labels': self.data_points[idx]['span_labels'][:len(span_indices)],\n",
    "            'data_for_metrics': {\n",
    "                'doc_id': self.data_points[idx]['doc_id'],\n",
    "                'hypothesis_id': self.data_points[idx]['hypothesis_id'],\n",
    "                'span_ids': span_ids,\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(os.path.join(cfg['raw_data_dir'], cfg['train_path']))\n",
    "dev_data = load_data(os.path.join(cfg['raw_data_dir'], cfg['dev_path']))\n",
    "test_data = load_data(os.path.join(cfg['raw_data_dir'], cfg['test_path']))\n",
    "\n",
    "hypothesis = get_hypothesis(train_data)\n",
    "\n",
    "train_data = train_data['documents']\n",
    "dev_data = dev_data['documents']\n",
    "test_data = test_data['documents']\n",
    "\n",
    "# train_data = train_data[:100]\n",
    "# dev_data = dev_data[:100]\n",
    "# test_data = test_data[:100]\n",
    "\n",
    "ic.disable()\n",
    "\n",
    "ic(len(train_data), len(dev_data), len(test_data))\n",
    "train_dataset = NLIDataset(train_data, tokenizer, hypothesis, [1000, 1100, 1200, 1500], 50)\n",
    "dev_dataset = NLIDataset(dev_data, tokenizer, hypothesis, [1000, 1100, 1200, 1500], 50)\n",
    "test_dataset = NLIDataset(test_data, tokenizer, hypothesis, [1000, 1100, 1200, 1500], 50)\n",
    "\n",
    "ic.enable()\n",
    "\n",
    "del train_data\n",
    "del dev_data\n",
    "del test_data\n",
    "del hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16166\n",
      "2471\n",
      "4799\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(dev_dataset))\n",
    "print(len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero_ct = 0\n",
    "# one_ct = 0\n",
    "\n",
    "# for x in train_dataset:\n",
    "#     if x['nli_label'] == 1 or x['nli_label'] == 2:\n",
    "#         for i in x['span_labels']:\n",
    "#             if i == 0:\n",
    "#                 zero_ct += 1\n",
    "#             else:\n",
    "#                 one_ct += 1\n",
    "\n",
    "# ic(zero_ct, one_ct)\n",
    "\n",
    "# zero_ct = 0\n",
    "# one_ct = 0\n",
    "\n",
    "# for x in dev_dataset:\n",
    "#     if x['nli_label'] == 1 or x['nli_label'] == 2:\n",
    "#         for i in x['span_labels']:\n",
    "#             if i == 0:\n",
    "#                 zero_ct += 1\n",
    "#             else:\n",
    "#                 one_ct += 1\n",
    "\n",
    "# ic(zero_ct, one_ct)\n",
    "\n",
    "# zero_ct = 0\n",
    "# one_ct = 0\n",
    "\n",
    "# for x in test_dataset:\n",
    "#     if x['nli_label'] == 1 or x['nli_label'] == 2:\n",
    "#         for i in x['span_labels']:\n",
    "#             if i == 0:\n",
    "#                 zero_ct += 1\n",
    "#             else:\n",
    "#                 one_ct += 1\n",
    "\n",
    "# ic(zero_ct, one_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # how many datapoints have no evidence\n",
    "# from collections import Counter\n",
    "\n",
    "# ic(Counter([x['nli_label'].item() for x in train_dataset]))\n",
    "# ic(Counter([x['nli_label'].item() for x in dev_dataset]))\n",
    "# ic(Counter([x['nli_label'].item() for x in test_dataset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmppy05uriu\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmppy05uriu/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "\n",
    "class ContractNLIConfig(PretrainedConfig):\n",
    "    def __init__(self, lambda_ = 1, bert_model_name = cfg['model_name'], num_labels = len(get_labels()), ignore_index = get_labels()['Ignore'], **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.bert_model_name = bert_model_name\n",
    "        self.num_labels = num_labels\n",
    "        self.ignore_index = ignore_index\n",
    "        self.lambda_ = lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from torch import nn\n",
    "\n",
    "class ContractNLI(PreTrainedModel):\n",
    "    config_class = ContractNLIConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = AutoModel.from_pretrained(config.bert_model_name)\n",
    "        self.bert.resize_token_embeddings(self.bert.config.vocab_size + 1, pad_to_multiple_of=8)\n",
    "        self.bert.eval()\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.embedding_dim = self.bert.config.hidden_size\n",
    "        self.num_labels = config.num_labels\n",
    "        self.lambda_ = config.lambda_\n",
    "        self.nli_criterion = nn.CrossEntropyLoss(ignore_index=config.ignore_index)\n",
    "        self.span_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        self.span_classifier = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, self.embedding_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.embedding_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "        self.nli_classifier = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, self.embedding_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.embedding_dim // 2, self.num_labels)\n",
    "        )\n",
    "\n",
    "        # initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" Initialize the weights \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            # use the same initialization as bert\n",
    "            module.weight.data.normal_(mean=0.0, std=self.bert.config.initializer_range)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, span_indices):\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids, output_hidden_states=True).hidden_states[-1]\n",
    "\n",
    "        gather = torch.gather(outputs, 1, span_indices.unsqueeze(2).expand(-1, -1, outputs.shape[-1]))\n",
    "\n",
    "        masked_gather = gather[span_indices != 0]\n",
    "        span_logits = self.span_classifier(masked_gather)\n",
    "        nli_logits = self.nli_classifier(outputs[:, 0, :])\n",
    "\n",
    "        return span_logits, nli_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np\n",
    "def get_micro_average_precision_at_recall(y_true, y_pred, recall_level):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    return np.interp(recall_level, recall[::-1], precision[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy and sklearn.metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_score\n",
    "def calculate_micro_average_precision(y_true, y_pred):\n",
    "    \"\"\"Calculate the micro average precision score.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.array): True labels.\n",
    "        y_pred (np.array): Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        float: Micro average precision score.\n",
    "    \"\"\"\n",
    "    # Get the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize the average precision score\n",
    "    average_precision = 0.0\n",
    "\n",
    "    # loop over all classes\n",
    "    for class_idx in range(num_classes):\n",
    "        # get the indices for this class\n",
    "        y_true_indices = np.where(y_true == class_idx)\n",
    "        # calculate the average precision score for this class\n",
    "        average_precision += ic(precision_score(\n",
    "            y_true[y_true_indices], y_pred[y_true_indices], average=\"micro\"\n",
    "        ))\n",
    "\n",
    "    # return the average over all classes\n",
    "    return average_precision / num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def calculate_f1_score_for_class(y_true, y_pred, class_idx):\n",
    "    \"\"\"Calculate the F1 score for a given class.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.array): True labels.\n",
    "        y_pred (np.array): Predicted labels.\n",
    "        class_idx (int): Index of the class.\n",
    "\n",
    "    Returns:\n",
    "        float: F1 score for the given class.\n",
    "    \"\"\"\n",
    "    # get the indices for the given class\n",
    "    y_true_indices = np.where(y_true == class_idx)\n",
    "    # calculate the F1 score for the given class\n",
    "    return f1_score(\n",
    "        y_true[y_true_indices], y_pred[y_true_indices], average=\"macro\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    auto_find_batch_size=True,\n",
    "    output_dir=cfg['results_dir'],   # output directory\n",
    "    num_train_epochs=10,            # total number of training epochs\n",
    "    gradient_accumulation_steps=4,   # number of updates steps to accumulate before performing a backward/update pass\n",
    "    logging_strategy='epoch',\n",
    "    # eval_steps=0.25,\n",
    "    # save_steps=0.25,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    label_names=['nli_label', 'span_labels', 'data_for_metrics'],\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/shu7bh/contract_nli/trained_model/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg['trained_model_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 253\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 364\n",
      "DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 513\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): storage.googleapis.com:443\n",
      "DEBUG:urllib3.connectionpool:https://storage.googleapis.com:443 \"GET /wandb-production.appspot.com/contract-nli-db/contract-nli/q6yqzpj9/artifact/625752025/wandb_manifest.json?Expires=1698929226&GoogleAccessId=gorilla-files-url-signer-man%40wandb-production.iam.gserviceaccount.com&Signature=k1CjNfv4oY2X149O9ac4PxAwhUuyMtEBOF05f8QDBJzVNSSMMD8%2BpEANrbGRUKQQlHoGwMmRMSazhWbu6vDLvhjdKS5nKMDEpTnRYTLPF70ShU447i9TA%2BP0cV895IAxxCpKOb6aLjuit5hBtZlxll3J5%2BLKfKhiE%2FZgTc%2BqmnbDEFpwnZ3gwtt5dtPP5w1%2BLZLyVX%2BPSTCBXfiN4hF9XqRxBZRiKcUWWhuOi7h5xBe9q2MVrxgnzIaGaVsYOXd27Kcl9QFN568ZSWVrxarJVAUB1rTMvasKjeDoK8afqe0IyjiV9UoGj0uiFpTc8hI7Zyqo6Lk9bD24q8mCed%2F3vA%3D%3D HTTP/1.1\" 200 684\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-q6yqzpj9:v0, 419.99MB. 3 files... \n",
      "DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 1252\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "Done. 0:0:3.2\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "api = wandb.Api()\n",
    "# artifact = api.artifact('contract-nli-db/contract-nli/model-3xmty1sv:v0', type='model')\n",
    "# artifact = api.artifact('contract-nli-db/contract-nli/model-ma3khgd5:v0', type='model')\n",
    "artifact = api.artifact('contract-nli-db/contract-nli/model-q6yqzpj9:v0', type='model')\n",
    "# artifact = api.artifact('contract-nli-db/contract-nli/model-fbypywbc:v0', type='model')\n",
    "\n",
    "\n",
    "\n",
    "# artifact = api.artifact('contract-nli-db/contract-nli/model-ayhbizbq:v0', type='model')\n",
    "# artifact = api.artifact('contract-nli-db/contract-nli/model-xa10s0tb:v0', type='model')\n",
    "# artifact = api.artifact('contract-nli-db/contract-nli/model-s3qw7z3d:v0', type='model')\n",
    "artifact_dir = artifact.download(cfg['trained_model_dir'])\n",
    "\n",
    "model = ContractNLI.from_pretrained(artifact_dir).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/shu7bh/contract_nli/trained_model/'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class ContractNLITrainer(Trainer):\n",
    "    def __init__(self, *args, data_collator=None, **kwargs):\n",
    "        super().__init__(*args, data_collator=data_collator, **kwargs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        span_label = inputs.pop('span_labels')\n",
    "        nli_label = inputs.pop('nli_label')\n",
    "        inputs.pop('data_for_metrics')\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        span_logits, nli_logits = outputs[0], outputs[1]\n",
    "\n",
    "        true_span_labels = []\n",
    "        pred_span_labels = []\n",
    "\n",
    "        start_index = 0\n",
    "        end_index = 0\n",
    "\n",
    "        for i, span_index_row in enumerate(inputs['span_indices']):\n",
    "            # find first zero index\n",
    "            first_zero_index = torch.where(span_index_row == 0)[0]\n",
    "\n",
    "            if len(first_zero_index) == 0:\n",
    "                first_zero_index = len(span_index_row)\n",
    "            else:\n",
    "                first_zero_index = first_zero_index[0].item()\n",
    "            end_index += first_zero_index\n",
    "\n",
    "            if nli_label[i] != get_labels()['Ignore']:\n",
    "                if nli_label[i] != get_labels()['NotMentioned']:\n",
    "                    true_span_labels.extend(span_label[start_index:end_index].tolist())\n",
    "                    pred_span_labels.extend(span_logits[start_index:end_index].tolist())\n",
    "\n",
    "            start_index = end_index\n",
    "\n",
    "        true_span_labels = torch.tensor(true_span_labels, dtype=torch.float32, device=DEVICE)\n",
    "        pred_span_labels = torch.tensor(pred_span_labels, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "        true_span_labels = true_span_labels.view(-1)\n",
    "        pred_span_labels = pred_span_labels.view(-1)\n",
    "\n",
    "        if len(true_span_labels) == 0 or len(pred_span_labels) != len(true_span_labels):\n",
    "            span_loss = torch.tensor(0, dtype=torch.float32, device=DEVICE)\n",
    "        else:\n",
    "            span_loss = self.model.span_criterion(pred_span_labels, true_span_labels)\n",
    "\n",
    "        nli_loss = self.model.nli_criterion(nli_logits, nli_label)\n",
    "\n",
    "        if torch.isnan(nli_loss):\n",
    "            nli_loss = torch.tensor(0, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "        if torch.isnan(span_loss):\n",
    "            span_loss = torch.tensor(0, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "        loss = span_loss + self.model.lambda_ * nli_loss\n",
    "\n",
    "        if loss.item() == 0:\n",
    "            loss = torch.tensor(0, dtype=torch.float32, device=DEVICE, requires_grad=True)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(features):\n",
    "        span_indices_list = [feature['span_indices'] for feature in features]\n",
    "        max_len = max([len(span_indices) for span_indices in span_indices_list])\n",
    "        span_indices_list = [torch.cat([span_indices, torch.zeros(max_len - len(span_indices), dtype=torch.long)]) for span_indices in span_indices_list]\n",
    "\n",
    "        span_ids_list = [feature['data_for_metrics']['span_ids'] for feature in features]\n",
    "        max_len = max([len(span_ids) for span_ids in span_ids_list])\n",
    "        span_ids_list = [torch.cat([span_ids, torch.zeros(max_len - len(span_ids), dtype=torch.long)]) for span_ids in span_ids_list]\n",
    "\n",
    "        input_ids = torch.stack([feature['input_ids'] for feature in features])\n",
    "        attention_mask = torch.stack([feature['attention_mask'] for feature in features])\n",
    "        token_type_ids = torch.stack([feature['token_type_ids'] for feature in features])\n",
    "        span_indices = torch.stack(span_indices_list)\n",
    "        nli_label = torch.stack([feature['nli_label'] for feature in features])\n",
    "        span_label = torch.cat([feature['span_labels'] for feature in features], dim=0)\n",
    "        data_for_metrics = {\n",
    "            'doc_id': torch.stack([feature['data_for_metrics']['doc_id'] for feature in features]),\n",
    "            'hypothesis_id': torch.stack([feature['data_for_metrics']['hypothesis_id'] for feature in features]),\n",
    "            'span_ids': torch.stack(span_ids_list),\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'span_indices': span_indices,\n",
    "            'nli_label': nli_label,\n",
    "            'span_labels': span_label,\n",
    "            'data_for_metrics': data_for_metrics,\n",
    "        }\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None):\n",
    "        ic(eval_dataset)\n",
    "        self.model.eval()\n",
    "        self.dataloader = ic(self.get_eval_dataloader(eval_dataset))\n",
    "        \n",
    "        true_span_labels = []\n",
    "        pred_span_labels = []\n",
    "        \n",
    "        true_nli_labels = []\n",
    "        pred_nli_labels = []\n",
    "\n",
    "        for inputs in tqdm(self.dataloader):\n",
    "            inputs = self._prepare_inputs(inputs)\n",
    "            ic(inputs['input_ids'].shape)\n",
    "            eval_span_labels = inputs.pop('span_labels')\n",
    "            eval_nli_labels = inputs.pop('nli_label')\n",
    "            inputs.pop('data_for_metrics')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                span_logits, nli_logits = outputs[0], outputs[1]\n",
    "\n",
    "                span_preds = ic(torch.sigmoid(span_logits)) >= 0.475\n",
    "                span_preds = torch.tensor(span_preds.squeeze(1), dtype=torch.long)\n",
    "                nli_preds = torch.argmax(torch.softmax(nli_logits, dim=1), dim=1)\n",
    "\n",
    "                start_index = 0\n",
    "                end_index = 0\n",
    "\n",
    "                for i, span_index_row in enumerate(inputs['span_indices']):\n",
    "                    # find first zero index\n",
    "                    first_zero_index = torch.where(span_index_row == 0)[0]\n",
    "\n",
    "                    if len(first_zero_index) == 0:\n",
    "                        first_zero_index = len(span_index_row)\n",
    "                    else:\n",
    "                        first_zero_index = first_zero_index[0].item()\n",
    "                    end_index += first_zero_index\n",
    "\n",
    "                    if eval_nli_labels[i] != get_labels()['NotMentioned']:\n",
    "                        true_span_labels.extend(eval_span_labels[start_index:end_index].tolist())\n",
    "                        pred_span_labels.extend(span_preds[start_index:end_index].tolist())\n",
    "\n",
    "                    true_nli_labels.append(eval_nli_labels[i].item())\n",
    "                    pred_nli_labels.append(nli_preds[i].item())\n",
    "\n",
    "                    start_index = end_index\n",
    "\n",
    "\n",
    "                # true_span_labels.extend(eval_span_labels.tolist())\n",
    "                # pred_span_labels.extend(span_preds.tolist())\n",
    "                \n",
    "                # true_nli_labels.extend(eval_nli_labels.tolist())\n",
    "                # pred_nli_labels.extend(nli_preds.tolist())\n",
    "            \n",
    "            # zip and print the true_span_labels and pred_span_labels together\n",
    "            # ic(list(zip(true_span_labels, pred_span_labels)))\n",
    "            # ic(list(zip(true_nli_labels, pred_nli_labels)))\n",
    "\n",
    "        eval_nli_acc = accuracy_score(true_nli_labels, pred_nli_labels)\n",
    "        \n",
    "        ic.enable()\n",
    "\n",
    "        ic(len(true_span_labels), len(pred_span_labels))\n",
    "\n",
    "        # print any span label if pred_span_label is 1\n",
    "        ic(sum(pred_span_labels), sum(true_span_labels))\n",
    "\n",
    "        mAP = calculate_micro_average_precision(torch.tensor(true_span_labels), torch.tensor(pred_span_labels))        \n",
    "\n",
    "        # mAP = average_precision_score(torch.tensor(true_span_labels), torch.tensor(pred_span_labels))\n",
    "        precision_at_80_recall = get_micro_average_precision_at_recall(torch.tensor(true_span_labels), torch.tensor(pred_span_labels), 0.8)\n",
    "        f1_score_for_entailment = calculate_f1_score_for_class(torch.tensor(true_nli_labels), torch.tensor(pred_nli_labels), get_labels()['Entailment'])\n",
    "        f1_score_for_contradiction = calculate_f1_score_for_class(torch.tensor(true_nli_labels), torch.tensor(pred_nli_labels), get_labels()['Contradiction'])\n",
    "        \n",
    "        return {\n",
    "            'mAP' : mAP,\n",
    "            'precision_at_80_recall' : precision_at_80_recall,\n",
    "            'nli_acc': eval_nli_acc,\n",
    "            'f1_score_for_entailment': f1_score_for_entailment,\n",
    "            'f1_score_for_contradiction': f1_score_for_contradiction\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ContractNLITrainer(\n",
    "    model=model,                          # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=dev_dataset,            # evaluation dataset\n",
    "    data_collator=ContractNLITrainer.collate_fn,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.001)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/309 [00:00<?, ?it/s]/tmp/ipykernel_26045/2581384782.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  span_preds = torch.tensor(span_preds.squeeze(1), dtype=torch.long)\n",
      "100%|██████████| 309/309 [00:50<00:00,  6.14it/s]\n",
      "ic| len(true_span_labels): 2797, len(pred_span_labels): 2797\n",
      "ic| sum(pred_span_labels): 1404, sum(true_span_labels): 1284\n",
      "ic| precision_score(\n",
      "        y_true[y_true_indices], y_pred[y_true_indices], average=\"micro\"\n",
      "    ): 0.4930601454064772\n",
      "ic| precision_score(\n",
      "        y_true[y_true_indices], y_pred[y_true_indices], average=\"micro\"\n",
      "    ): 0.4961059190031153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mAP': 0.49458303220479627,\n",
       " 'precision_at_80_recall': 0.4569360182099378,\n",
       " 'nli_acc': 0.5880210441116956,\n",
       " 'f1_score_for_entailment': 0.2794649313087491,\n",
       " 'f1_score_for_contradiction': 0.1797752808988764}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic.disable()\n",
    "results = trainer.evaluate()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/309 [00:00<?, ?it/s]/tmp/ipykernel_26045/1825388109.py:125: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  span_preds = torch.tensor(span_preds.squeeze(1), dtype=torch.long)\n",
      "100%|██████████| 309/309 [00:33<00:00,  9.11it/s]\n",
      "ic| len(true_span_labels): 2797, len(pred_span_labels): 2797\n",
      "ic| sum(pred_span_labels): 18, sum(true_span_labels): 1284\n",
      "ic| precision_score(\n",
      "        y_true[y_true_indices], y_pred[y_true_indices], average=\"micro\"\n",
      "    ): 0.9940515532055518\n",
      "ic| precision_score(\n",
      "        y_true[y_true_indices], y_pred[y_true_indices], average=\"micro\"\n",
      "    ): 0.007009345794392523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mAP': 0.5005304494999722,\n",
       " 'precision_at_80_recall': 0.467308418683884,\n",
       " 'nli_acc': 0.5880210441116956,\n",
       " 'f1_score_for_entailment': 0.2794649313087491,\n",
       " 'f1_score_for_contradiction': 0.1797752808988764}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic.disable()\n",
    "results = trainer.evaluate()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
